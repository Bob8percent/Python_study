{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "#   データAPI\n",
    "\n",
    "#   RAM上にデータセット(データ要素のシーケンス)を作る\n",
    "X = tf.range(10)    #   [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X) #   10個の要素を含むデータセット\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#dataset = dataset.repeat(3).batch(7)    #   repeat(3)で3回繰り返し、batch(7)で7個ずつのバッチにまとめる\n",
    "                                        #   batch(7, drop_remainder=True)で端数バッチをのぞける\n",
    "                                        #   detasetメソッドはデータを書き換えずに新しいdatasetを作る\n",
    "                                        \n",
    "#dataset = dataset.map(lambda x: x*2)  #   個々の要素を2倍する。適用する関数はtensorflow関数に変換できるものに限定される\n",
    "\n",
    "#dataset = dataset.apply(tf.data.experimental.unbatch()) #   データセット全体に対する変換\n",
    "\n",
    "#dataset = dataset.filter(lambda x: x<10)    #   10未満の要素にフィルタをかける\n",
    "\n",
    "# for item in dataset.take(3):   #   take(n)はデータの一部(n個)のみを使う\n",
    "#     print(item)\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size=1, seed=42).batch(7)  #   buffer_size=1ならシャッフルされない\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   複数ファイルの行のインターリーブ\n",
    "train_filepaths = \"datasets/housing/my_train_*.csv\"\n",
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42) #   ファイルパスをシャッフルしたデータセットを得られる\n",
    "\n",
    "n_readers = 5\n",
    "dataset = filepath_dataset.interleave(  #   n_reader個ずつファイパスを抜き出し引数の関数を実行を繰り返し新しいデータセットを返す\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1), #   先頭行を読み飛ばして一行ずつ読む\n",
    "    cycle_length=n_readers\n",
    ")\n",
    "\n",
    "#   データの前処理\n",
    "X_mean, X_std = [...]   #   訓練セットの各特徴量の平均と標準偏差(教科書の例では要素数は8個)\n",
    "n_inputs = 8\n",
    "\n",
    "def preprocess(line):\n",
    "    defs = [0.]*n_inputs + [tf.constant([], dtype=tf.float32)]  #   要素数n_inputs個のtf.float32型の空配列\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(fields[:-1])   #   すべての要素を1次元配列につめこむ\n",
    "    y = tf.stack(fields[-1:])   #   最後の要素だけ\n",
    "    return (x - X_mean) / X_std, y  #   スケーリング\n",
    "\n",
    "\n",
    "#   1つにまとめる\n",
    "def csv_reader_dataset(filepaths, repeat=1, n_readers=5, \n",
    "                        n_read_threads=None, shuffle_buffer_size=10000,\n",
    "                        n_parse_threads=5, batch_size=32):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers, num_parallel_calls=n_parse_threads  #   マルチスレッド化\n",
    "    )\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)   #   マルチスレッド化\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size).repeat(repeat)\n",
    "    return dataset.batch(batch_size).prefetch(1)    #   prefetchによって常に1バッチ先を準備することにより処理速度を上げる\n",
    "\n",
    "train_set = csv_reader_dataset(train_filepaths)\n",
    "valid_set = csv_reader_dataset(valid_filepaths)\n",
    "test_set = csv_reader_dataset(test_filepaths)\n",
    "\n",
    "model = keras.models.Sequential([...])\n",
    "model.compile([...])\n",
    "model.fit(train_set, epochs=10, validation_data=valid_set)  #   model.fitにdatasetを渡せる\n",
    "model.evaluate(test_set)    #   model.evaluateにも渡せる\n",
    "new_set = test_set.take(3).map(lambda X, y:X)\n",
    "model.predict(new_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#   TFRecord形式 : データのロード、パースが訓練のボトルネックとなるときに有効\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mTFRecordWriter(\u001b[39m\"\u001b[39m\u001b[39mmy_data.tfrecord\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m      3\u001b[0m     f\u001b[39m.\u001b[39mwrite(\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThis is the first record\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m     f\u001b[39m.\u001b[39mwrite(\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAnd this is the second record\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "#   TFRecord形式 : データのロード、パースが訓練のボトルネックとなるときに有効\n",
    "with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n",
    "    f.write(b\"This is the first record\")    #   バイナリ形式の書き込み\n",
    "    f.write(b\"And this is the second record\")\n",
    "    \n",
    "filepaths = [\"my_data.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filepaths)    #   あとはcsvと同様に処理可能\n",
    "\n",
    "#   TFRecordファイル形式の圧縮\n",
    "options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
    "with tf.io.TFRecordWriter(\"my_compressed.tfrecord\", options) as f:\n",
    "    [...]\n",
    "    \n",
    "dataset = tf.data.TFRecordDataset([\"my_compressed.tfrecord\"], compression_type=\"GZIP\")  #   ファイルの読み込み\n",
    "\n",
    "#   プロトコルバッファ : TFRecordファイルの個々のレコードは通常プロトコルバッファ\n",
    "# syntax = \"proto3\";\n",
    "# message Person{\n",
    "#     string name = 1;    #   バイナリ表現で使用されるfield IDを設定\n",
    "#     int32 d = 2;\n",
    "#     repeated string email = 3;\n",
    "# }\n",
    "#   from person_pb2 import Person\n",
    "person = Person(name=\"Al\", id=123, email=[\"a@b.com\"])   #   Personを作る\n",
    "person.name             #   \"Al\"\n",
    "person.name = \"Alice\"   #   書き換え\n",
    "person.email[0]         #   \"a@b.com\"\n",
    "person.email.append(\"c@d.com\")  #   追加\n",
    "s = person.SerializeToString()  #   オブジェクトをバイト列にシリアライズ(一連の形式にする)。\n",
    "person2 = Person()  #   新しいPerson\n",
    "person2.ParseFromString(s)  #   sをパースできる.tf関数に組み込むにはtf.py_function()でラップする必要がある\n",
    "person == person2   #   True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   TFRecordでは主にExample protobufで表現される\n",
    "# syntax = \"proto3\";\n",
    "# message BytesList{ repeated bytes value = 1; }\n",
    "# message FloatList{ repeated float value = 1 [packed = true]; }    #   [packed=true]でエンコードを効率化できるらしい\n",
    "# message Int64List{ repeated int64 value = 1 [packed = true]; }\n",
    "# message Feature{\n",
    "#     oneof kind{ #   3つのリストのどれか\n",
    "#         BytesList bytes_list = 1;\n",
    "#         FloatList float_list = 2;\n",
    "#         Int64List int64_list = 3;\n",
    "#     }\n",
    "# }\n",
    "# message Features{ map<string, Feature> feature = 1; }\n",
    "# message Example{ Features features = 1; }\n",
    "from tensorflow.train import BytesList, FloatList, Int64List\n",
    "from tensorflow.train import Feature, Features, Example\n",
    "#   tf.train.Exampleを使ってTFRecordファイルに書き込む\n",
    "person_example = Example(\n",
    "    features=Features(\n",
    "        feature={\n",
    "            \"name\": Feature(bytes_list=BytesList(value=[b\"Alice\"]))\n",
    "            \"id\":   Feature(int64_list=Int64List(value=[123])),\n",
    "            \"emails\":   Feature(bytes_list=BytesList(value=[b\"a@b.com\", b\"c@d.com\"]))\n",
    "        }\n",
    "    )\n",
    ")\n",
    "with tf.io.TFRecordWriter(\"my_contacts.tfrecord\") as f:\n",
    "    f.write(person_example.SerializeToString())\n",
    "\n",
    "#   Exampleのロードとパース\n",
    "feature_description = { #   記述辞書\n",
    "    \"name\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"), #   固定長特徴量\n",
    "    \"id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    \"emails\":   tf.io.VarLenFeature(tf.stirng)  #   emailsは可変\n",
    "}\n",
    "for serialized_example in tf.data.TFRecordDataset([\"my_contats.tfrecord\"]): #   ロード\n",
    "    parsed_example = tf.io.parse_single_example(serialized_example, feature_description)   #   個々のExampleをパース\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   SequenceExample protobufを使用したリスト処理\n",
    "# message FeatureList{ repeated Feature feature = 1; }\n",
    "# message FeatureLists{ map<string, FeatureList> feature_list = 1; }\n",
    "# message SequenceExample{\n",
    "#     Features context = 1;\n",
    "#     FeatureLists feature_lists = 2;\n",
    "# }\n",
    "\n",
    "parsed_context, parsed_feature_lists = tf.io.parse_single_sequence_example( #   1つのSequenceExampleのパース\n",
    "    serialized_sequence_example, context_feature_descriptions,\n",
    "    sequence_feature_descriptions\n",
    ")\n",
    "parsed_content = tf.RaggedTensor.from_sparse(parsed_feature_lists[\"content\"])\n",
    "\n",
    "#   前処理の方法の一つとして前処理層を挿入する\n",
    "means = np.mean(X_train, axis=0, keepdims=True)\n",
    "stds = np.std(X_train, axis=0, keepdims=True)\n",
    "eps = keras.backend.epsilon()\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Lambda(lambda inputs: (inputs-means)/(stds+eps)),\n",
    "    [...]\n",
    "])\n",
    "\n",
    "class Standardization(keras.layers.Layer):  #   カスタムレイヤとしてもOK\n",
    "    def adapt(self, data_sample):\n",
    "        self.means_ = np.mean(data_sample, axis=0,keepdims=True)\n",
    "        self.stds_ = np.std(data_sample, axis=0, keepdims=True)\n",
    "    def call(self, inputs):\n",
    "        return (inputs-self.means_)/(self.stds_+keras.backend.epsilon())\n",
    "std_layer = Standardization()\n",
    "std_layer.adapt(data_sample)\n",
    "model = keras.Sequential()\n",
    "model.add(std_layer)\n",
    "[...]   #   他レイヤ\n",
    "model.compile([...])\n",
    "model.fit([...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#   ワンホットベクトルを使用したカテゴリ特徴量のエンコード\u001b[39;00m\n\u001b[0;32m      2\u001b[0m vocab \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m<1H OCEAN\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mINRAND\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mNEAR OCEAN\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mNEAR BAY\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mISLAND\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m#   カテゴリのリスト\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m indices \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mrange(\u001b[39mlen\u001b[39m(vocab), dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mint64)  \u001b[39m#   インデックスに対応するテンソルを作る\u001b[39;00m\n\u001b[0;32m      4\u001b[0m table_init \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mlookup\u001b[39m.\u001b[39mKeyValueTensorInitializer(vocab, indices) \n\u001b[0;32m      5\u001b[0m num_oov_buckets \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m    \u001b[39m#   語彙外バケットの数を指定。カテゴリの数が変化するというときの対処法になりうる\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "#   ワンホットベクトルを使用したカテゴリ特徴量のエンコード : カテゴリ数が少ないときに有効\n",
    "vocab = [\"<1H OCEAN\", \"INRAND\", \"NEAR OCEAN\", \"NEAR BAY\", \"ISLAND\"] #   カテゴリのリスト\n",
    "indices = tf.range(len(vocab), dtype=tf.int64)  #   インデックスに対応するテンソルを作る\n",
    "table_init = tf.lookup.KeyValueTensorInitializer(vocab, indices) \n",
    "num_oov_buckets = 2    #   語彙外バケットの数を指定。カテゴリの数が変化するというときの対処法になりうる\n",
    "table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)\n",
    "\n",
    "categories = tf.constant([\"NEAR BAY\", \"DESERT\", \"INLAND\", \"INLAND\"])\n",
    "cat_indices = table.lookup(categories)  #   [3,5,1,1]\n",
    "cat_one_hot = table.one_hot(cat_indices, depth=len(vocab)+num_oov_buckets)  #   ワンホットエンコーディング。[[0,0,0,1,0,0,0],[...]]\n",
    "\n",
    "#   埋め込み(ベクトルを使ってカテゴリを表現する方法)を使用したカテゴリ特徴量のエンコード : カテゴリ数が多いときに有効\n",
    "embedding_dim = 2   #   2次元ベクトル\n",
    "embed_init = tf.random.uniform([len(vocab) + num_oov_buckets, embedding_dim])\n",
    "embedding_matrix = tf.Variable(embed_init)  #   6*2行列\n",
    "categories = tf.constant([\"NEAR BAY\", \"DESERT\", \"INLAND\", \"INLAND\"])\n",
    "cat_indices = table.lookup(categories)\n",
    "tf.nn.embedding_lookup(embedding_matrix, cat_indices)   #   4*2行列。4つのカテゴリに対応する2次元ベクトル\n",
    "embedding = keras.layers.Embedding(input_dim=len(vocab)+num_oov_buckets, output_dim=embedding_dim)  #   埋め込み行列を処理するEmbedding層\n",
    "embedding(cat_indices)\n",
    "#   埋め込み学習\n",
    "regular_inputs = keras.layers.Input(shape=[8])  #   数値特徴量を含む通常入力\n",
    "categories = keras.layers.Input(shape=[], dtype=string)\n",
    "cat_indices = keras.layers.Lambda(lambda cats: table.lookup(cats))(categories)\n",
    "cat_embed = keras.layers.Embedding(input_dim=6, output_dim=2)(cat_indices)  #   カテゴリ入力\n",
    "encoded_inputs = keras.layers.concatenate([regular_inputs, cat_embed])  #   以上を連結\n",
    "outputs = keras.layers.Dense(1)(encoded_inputs)\n",
    "model = keras.models.Model(inputs=[regular_inputs, categories], outputs=[outputs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   keras前処理層の詳細\n",
    "normalization = keras.layers.Normalization()    #   正規化\n",
    "discretization = keras.layers.Discretization([...]) #   離散化\n",
    "pipeline = keras.layers.PreprocessingStage([normalization, discretization])    #   複数の前処理層をつなげることができる\n",
    "pipeline.adapt(data_sample)\n",
    "\n",
    "#   Apache Beamによる効率上昇、TF Transformによる保守の簡易化\n",
    "import tensorflow_transform as tft\n",
    "def preprocess(inputs): #   Featuresの入力バッチ\n",
    "    median_age = inputs[\"housing_median_age\"]\n",
    "    ocean_proximity = inputs[\"ocean_proximity\"]\n",
    "    standardized_age = tft.scale_to_z_score(median_age)\n",
    "    ocean_proximity_id = tft.compute_and_apply_vocabulary(ocean_proximity)\n",
    "    return {\n",
    "        \"standardized_median_age\": standardized_age,\n",
    "        \"ocean_proximity_id\": ocean_proximity_id\n",
    "    }\n",
    "\n",
    "#   MNISTなど標準データセットが使えればいいのであればTFDSを使う\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "dataset  = tfds.load(name=\"mnist\")  #   データのダウンロード.特徴量とラベルの辞書になっている\n",
    "mnist_train, mnist_test = dataset[\"train\"], dataset[\"test\"]\n",
    "mnist_train = mnist_train.shuffle(10000).batch(32).prefetch(1)\n",
    "for item in mnist_train:\n",
    "    images = item[\"image\"]\n",
    "    labels = item[\"label\"]\n",
    "    [...]\n",
    "    \n",
    "mnist_train = mnist_train.shuffle(10000).batch(32)\n",
    "mnist_train = mnist_train.map(lambda items: (items[\"image\"], items[\"label\"]))   #   kerasは個々の要素が2要素tupleとなることを前提としているので、\n",
    "                                                                                #   2要素tupleに変換\n",
    "mnist_train = mnist_train.prefetch(1)\n",
    "\n",
    "#   上の方法よりも簡単な方法\n",
    "dataset = tfds.load(name=\"mnist\", batch_size=32, as_supervised=True)    #   as_supervised=Trueによって変換する\n",
    "mnist_train = dataset[\"train\"].prefetch(1)\n",
    "model = keras.Sequential([...])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\")\n",
    "model.fit(mnist_train, epoch=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a6fdf0cebff75ebc8562d1fd6dfb543976b20263b8527b00aa8ceb00671f916a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
