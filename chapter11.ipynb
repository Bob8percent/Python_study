{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "#   勾配の不安定性を取り除く方法\n",
    "#   1. 重み初期化法\n",
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")   #   正規分布でのHe初期化\n",
    "\n",
    "he_avg_init = keras.initializers.VarianceScaling(scale=2., mode=\"fan_avg\", distribution=\"uniform\")  #   一様分布でのHe初期化かつfan_avgを利用\n",
    "keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)\n",
    "\n",
    "#   2. 様々な活性化関数\n",
    "model = keras.models.Sequential([\n",
    "    [...]\n",
    "    keras.layers.Dense(10, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(alpha=0.2),   #   LeakyReLU関数を適用したい層のすぐ後ろに追加\n",
    "    [...]\n",
    "])\n",
    "\n",
    "layer = keras.layers.Dense(10, activation=\"selu\", kernel_initialier=\"lecun_normal\") #   SELU関数を適用する方法\n",
    "\n",
    "#   3. バッチ正規化\n",
    "\n",
    "#   活性化関数の後ろにBN層\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),                                      #   以下パラメータ数について\n",
    "    keras.layers.BatchNormalization(),                                              #   28*28*4(BN層により入力当たり4つのパラメータ追加)\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),      #   28*28*300+300\n",
    "    keras.layers.BatchNormalization(),                                              #   300*4\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),      #   300*100+100\n",
    "    keras.layers.BatchNormalization(),                                              #   100*4\n",
    "    keras.layers.Dense(10, activation=\"softmax\")                                    #   100*10+10\n",
    "])\n",
    "\n",
    "#   活性化関数の前にBN層\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),                                      \n",
    "    keras.layers.BatchNormalization(),                                              \n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),        #   BN層は入力ごとに1個のオフセットパラメータ(バイアス)を持っているのでuse_bias=Falseでバイアス項を取り除く      \n",
    "    keras.layers.BatchNormalization(),                                         \n",
    "    keras.layers.Activation(\"elu\"),                                                 #   活性化関数を分離して記述     \n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\", use_bias=False),      \n",
    "    keras.layers.BatchNormalization(),                                              \n",
    "    keras.layers.Dense(10, activation=\"softmax\")                                    \n",
    "])\n",
    "\n",
    "#   4. 勾配クリッピング : バッチ正規化が使いにくい場合の代替手段となる\n",
    "optimizer = keras.optimizers.SGD(clipvalue=1.0) #   勾配ベクトルを-1.0~1.0にクリッピング。clipnorm=1.0と指定すれば向きが変わらない。\n",
    "model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   転移学習 : データセットが少ない時、既存のモデルを再利用して対処\n",
    "model_A = keras.model.load_model(\"my_model_A.h5\")   #   model_Aは8種類に分類するよう訓練したモデルと想定\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1]) #   model_Aの出力層以外をすべて再利用するモデルを定義\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))   #   ここでは二値分類を想定しているので、出力層は1つ\n",
    "\n",
    "model_A_clone = keras.models.clone_model(model_A)   #   訓練によってmodel_Aの更新を避けたければこのようにクローンを作ってから再利用\n",
    "model_A_clone.set_weights(model_A.get_weights())\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1]:  #   最初の数エポックは再利用層を凍結して新しい層に妥当な重みを学習するための時間を与える\n",
    "    layer.trainable = False\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"]) #   凍結、凍結解除時はコンパイルが必要\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4, validation_data=(X_valid_B, y_valid_B))\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True  #   凍結解除\n",
    "optimizer = keras.model.optimizer.SGD(lr=1e-4)  #   再利用層を壊さないために学習率を下げる\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16, validation_data=(X_valid_B, y_valid_B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   optimizerの高速化\n",
    "\n",
    "#   1. モーメンタム最適化 : 勾配降下法に加えて、加速度を導入する\n",
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)    #   重みの更新量を速度、momentumを加速度と考える。\n",
    "\n",
    "#   2. NAG : モーメンタム最適化に加えて、運動量が働く方向に少し進んだ値(少し正確な値)を勾配測定に使用する\n",
    "optimizer = keras.optimizer.SGD(lr=0.001, momentum=0.9, nesterov=True)\n",
    "\n",
    "#   3. RMSProp : AdaGrad(最適値に近づくほど学習率(速度)を下げていく手法)のスローダウン問題を解決した手法\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)\n",
    "\n",
    "#   4. Adam : モーメンタム最適化とRMSPropの組み合わせ\n",
    "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 8 (1815707354.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [2], line 9\u001b[1;36m\u001b[0m\n\u001b[1;33m    return lr0 * 0.1**(epoch/s)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after function definition on line 8\n"
     ]
    }
   ],
   "source": [
    "#   学習率のスケジューリング\n",
    "\n",
    "#   1. パワースケジューリング : 減少加速度がどんどん小さくしていく\n",
    "optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-4)   #   lrは初期学習率、decayはステップ数の逆数\n",
    "\n",
    "#   2. 指数スケジューリング : sステップごとに1/10になっていく\n",
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1**(epoch/s)\n",
    "    return exponential_decay_fn\n",
    "exponential_decay_fn = exponential_decay(0.01, 20)\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)  #   各エポックの冒頭で学習率を更新\n",
    "history = model.fit(X_train_scaled, y_train, [...], callbacks=[lr_scheduler])   #   モデルの保存時、epochは保存されないので注意\n",
    "                                                                                #   この問題はfit()のinitial_epochを適切に設定すれば解決する\n",
    "#   3. 部分ごとに一定の学習率を決めておくスケジューリング\n",
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(piecewise_constant_fn)\n",
    "history = model.fit([...])\n",
    "\n",
    "#   4. 性能によるスケジューリング\n",
    "#   5エポック連続で最良の検証損失が向上しなければ学習率に0.5をかける\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "\n",
    "#   よりシンプルな設定方法\n",
    "s = 20 * len(X_train)   #   20epoch実行したときのステップ数\n",
    "learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\n",
    "optimizer = keras.optimizers.SGD(learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   正則化テクニック : ロス関数に重みの絶対値に比例して大きくなる正則化項を加えた関数の最小化を考えることで過学習を防ぐ\n",
    "\n",
    "#   1. l1, l2正則化\n",
    "layer = keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\", \n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))  #   正則化率0.01のl2正則化\n",
    "\n",
    "#   modelを作成するときにデフォルト引数をあらかじめ設定しておく方法\n",
    "from functools import partial\n",
    "RegularizedDense = partial(keras.layers.Dense, activation=\"elu\", kernel_initializer=\"he_normal\", \n",
    "                           kernel_regularizer=keras.regularizer.l2(0.01))\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation=\"softmax\", kernel_initializer=\"glorot_uniform\")\n",
    "])\n",
    "\n",
    "#   2. ドロップアウト : 訓練中、rateの確率でニューロンを無視する。訓練後に重みに(1-p)倍するか、訓練中に(1-p)で割るかを行わなければいけない\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "#   3. MCドロップアウト\n",
    "y_probas = np.stack([model(X_test_scaled, training=True)    #   training=Trueでドロップアウト層をアクティブに\n",
    "                     for sample in range(100)]) #   ドロップアウト訓練済みのモデルを使用して100回(MCサンプル数)テストした結果をスタックする\n",
    "y_proba = y_probas.mean(axis=0)\n",
    "\n",
    "#   4. 重み上限正則化\n",
    "keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\", \n",
    "                   kernel_constraint=keras.constraints.max_norm(1.))    #   l2ノルム <= max_norm に制限"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a6fdf0cebff75ebc8562d1fd6dfb543976b20263b8527b00aa8ceb00671f916a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
