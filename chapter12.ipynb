{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=43.0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "#   テンソルの作り方 : tf.Tensorはimmutable\n",
    "tf.constant([[1.,2.,3.],[4.,5.,6.]])    #   2*3行列\n",
    "tf.constant(42)                         #   スカラー\n",
    "\n",
    "t = tf.constant([[1.,2.,3.],[4.,5.,6.]])\n",
    "t.shape #   TensorShape([2, 3])\n",
    "t.dtype #   tf.float32\n",
    "\n",
    "t[:, 1:]    #   numpyと同様、[[2.,3.],[5.,6.]]\n",
    "t[..., 1, tf.newaxis]   #   [[2.],[5.]]\n",
    "t + 10  #   [[11., 12., 13.], [14., 15., 16.]]\n",
    "tf.square(t)    #   [[ 1.,  4.,  9.],[16., 25., 36.]]\n",
    "t @ tf.transpose(t) #   tとtの転置の行列積\n",
    "\n",
    "#   テンソルとNumpy\n",
    "#   デフォルトでは、numpyは64bit精度、tensorflowは32bit精度\n",
    "a = np.array([2.,4.,5.])\n",
    "tf.constant(a)\n",
    "t.numpy()   #   np.array(t)\n",
    "np.square(t)\n",
    "\n",
    "#   tensorflowは暗黙の型変換をサポートしない(処理性能を下げないためのよう)\n",
    "tf.constant(3.) + tf.constant(40)                   #   error\n",
    "tf.constant(3.) + tf.constant(40, dtype=tf.float32) #   OK\n",
    "\n",
    "t2 = tf.constant(40)\n",
    "tf.constant(3.) + tf.cast(t2, tf.float32)   #   OK : cast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[100.,  42.,   0.],\n",
       "       [  8.,  10., 200.]], dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#   変数 : tf.Tensorと同様にふるまう\n",
    "v = tf.Variable([[1.,2.,3.],[4.,5.,6.]])\n",
    "\n",
    "#   tf.Tensorと異なり値を書き換えられる\n",
    "v.assign(2*v)       #   [[ 2.,  4.,  6.],[ 8., 10., 12.]]\n",
    "v[0,1].assign(42)   #   [[ 2., 42.,  6.],[ 8., 10., 12.]]\n",
    "v[:,2].assign([0.,1.])  #   [[ 2., 42.,  0.],[ 8., 10.,  1.]]\n",
    "v.scatter_nd_update(indices=[[0,0], [1,2]], updates=[100.,200.])    #   [[100.,  42.,   0.],[  8.,  10., 200.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=int64, numpy=\n",
       "array([[0],\n",
       "       [2]], dtype=int64)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#   カスタム損失関数\n",
    "def huber_fn(y_true, y_pred):   #   y_true : ラベル, y_pred : 予測\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "model.compile(loss=huber_fn, optimizer=\"nadam\")\n",
    "model.fit(X_train, y_train, [...])\n",
    "\n",
    "#   上のようなカスタムコンポーネントを含むモデルの保存とロード\n",
    "model = keras.models.load_model(\"my_model_with_a_custom_loss.h5\", custom_objects={\"huber_fn\": huber_fn})    #   ロードのために辞書が必要\n",
    "\n",
    "#   閾値を変えたいとき\n",
    "def create_huber(threshold=1.0):\n",
    "    def huber_fn(y_true, y_pred):   #   y_true : ラベル, y_pred : 予測\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = threshold * tf.abs(error) - threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    return huber_fn\n",
    "\n",
    "model.compile(loss=create_huber(2.0), optimizer=\"nadam\")\n",
    "model = keras.models.load_model(\"my_model_with_a_custom_loss_threshold_2.h5\",\n",
    "                                custom_objects={\"huber_fn\": create_huber(2.0)}) #   modelの保存時、thresholdが保存されないので、ロード時に指定する必要がある\n",
    "\n",
    "\n",
    "#   keras.losses.Lossのサブクラスを作りget_configを利用することでロード時の値の設定は必要なくなる(オーバーライド関数を編集して引数としてオブジェクトを渡すだけでよい)\n",
    "class HuberLoss(keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super.__init__(**kwargs)    #   標準ハイパーパラメータを処理\n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()  #   ハイパーパラメータを持つ辞書を取得\n",
    "        return {**base_config, \"threshold\": self.threshold} #   thresholdを追加した辞書を返す\n",
    "    \n",
    "model.compile(loss=HuberLoss(2.), optimizer=\"nadam\")\n",
    "model = keras.models.load_model(\"my_model_with_a_custom_loss_class.h5\", custom_objects={\"HuberLoss\": HuberLoss})\n",
    "\n",
    "#   一般的なカスタムコンポーネント\n",
    "#   カスタム活性化関数\n",
    "def my_softplus(z):\n",
    "    return tf.math.log(tf.exp(z) + 1.0)\n",
    "\n",
    "#   カスタム初期化子\n",
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(1. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "#   カスタム正則化器\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))    #   l1ノルム\n",
    "\n",
    "#   カスタム制約\n",
    "def my_positive_weights(weights):\n",
    "    return tf.where(weights < 0., tf.zero_like(weights), weights)\n",
    "\n",
    "layer = keras.layers.Dense(30, activation=my_softplus,\n",
    "                           kernel_initializer=my_glorot_initializer,\n",
    "                           kernel_regularizer=my_l1_regularizer,\n",
    "                           kernel_constraint=my_positive_weights)\n",
    "\n",
    "#   ハイパーパラメータを持つカスタムコンポーネントの定義にはサブクラス化を利用\n",
    "class MyL1Regularizer(keras.regularizers.Regularizer):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "    def __call__(self, weights):    #   正則化器、初期化子、制約ではcall()ではなく__call__メソッドの定義が必要\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "    def get_config(self):\n",
    "        return {\"factor\": self.factor}\n",
    "    \n",
    "#   カスタム指標\n",
    "#   単に指標の平均を得たい場合\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[create_huber(2.0)])\n",
    "\n",
    "#   ストリーミング指標(全体の指標を逐一再計算する.正しい指標が得られる)\n",
    "#   Precisionオブジェクトはストリーミング指標をを計算している好例\n",
    "precision = keras.model.Precision()\n",
    "precision([0,1,1,1,0,1,0,1],[1,1,0,1,0,1,0,1])  #バッチのラベルと予測を渡す\n",
    "precision([0,1,0,0,1,0,1,1],[1,0,1,1,0,0,0,0])\n",
    "precision.result()  #   現時点での指標\n",
    "precision.valiables #   オブジェクトが管理する変数(指標計算に用いる変数)\n",
    "precision.reset_states()    #   変数をリセット\n",
    "\n",
    "#   ストリーミング指標を得たいときはkeras.metrics.Metricをサブクラス化\n",
    "class HuberMetric(keras.metrrics.Metric):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold)\n",
    "        self.total = self.add_weight(\"total\", initializer=\"zeros\")  #   変数を作る\n",
    "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        self.total.assign_add(tf.reduce_sum(metric))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "    def result(self):\n",
    "        return self.total / self.count\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}\n",
    "    #   reset_states()はデフォルトでは変数を0にリセットするが、必要ならオーバーライドできる\n",
    "    \n",
    "#   カスタムレイヤ\n",
    "#   kers.layers.Flatttenのような重みをもたないカスタムレイヤを作りたいときはkeras.layers.Lambdaを利用\n",
    "exponential_layer = keras.layers.Lambda(lambda x: tf.exp(x))\n",
    "\n",
    "#   重みをもつレイヤを作りたい\n",
    "class MyDense(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units  #   ニューロンの数\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        \n",
    "    def build(self, batch_input_shape): #   レイヤが初めて使用されるときに呼び出される\n",
    "        self.kernel = self.add_weight(  #   接続重み行列\n",
    "            name=\"kernel\", shape=[batch_input_shape[-1], self.units],\n",
    "            initializer=\"glorot_normal\"\n",
    "        )\n",
    "        self.bias = self.add_weight(    #   バイアス項\n",
    "            name=\"bias\", shape=[self.units], initializer=\"zeros\"\n",
    "        )\n",
    "        super().build(batch_input_shape)\n",
    "        \n",
    "    def call(self, X):\n",
    "        return self.activation(X @ self.kernel + self.bias) #   レイヤの出力。Xは入力\n",
    "    \n",
    "    def compute_output_shape(self, batch_input_shape):  #   レイヤの出力のshape。省略できる\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:,-1] + [self.units]) #   as_list()によってPythonリストに変換できる\n",
    "    \n",
    "    def get_config(self):   #   他のカスタムクラスと同様、設定を保存する\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"units\": self.units,\n",
    "                \"activation\": keras.activations.serialize(self.activation)}\n",
    "        \n",
    "#   複数入力(concatenate)を持つレイヤを作りたい\n",
    "class MyMultiLayer(keras.layers.layer):\n",
    "    def call(self, X):  #   Xはすべての入力を含むtuple\n",
    "        X1, X2 = X\n",
    "        return [X1 + X2, X1 * X2, X1 / X2]\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        b1, b2 = batch_input_shape\n",
    "        return [b1, b1, b1]\n",
    "    \n",
    "#   BN層やDropout層など、訓練中とテスト中でレイヤのふるまいを変えたいとき、call()にtraining引数を追加する\n",
    "class MyGaussianNoise(keras.layers.layer):\n",
    "    def __init__(self, stddev, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "    \n",
    "    def call(self, X ,training=None):\n",
    "        if training:\n",
    "            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n",
    "            return X + noise\n",
    "        else:\n",
    "            return X\n",
    "    \n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return batch_input_shape\n",
    "    \n",
    "#   カスタムモデル\n",
    "#   まずカスタムレイヤを定義\n",
    "class ResidualBlock(keras.layers.layer):\n",
    "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hiddens = [keras.layers.Dense(n_neurons, activation=\"elu\", kernel_initializer=\"he_normal\") for _ in range(n_layers)]\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hiddens:\n",
    "            Z = layer(Z)\n",
    "        return inputs + Z\n",
    "    \n",
    "class ResidualRegressor(keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\")\n",
    "        self.block1 = ResidualBlock(2, 30)\n",
    "        self.block2 = ResidualBlock(2, 30)\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        Z = self.hidden1(inputs)\n",
    "        for _ in range(1 + 3):\n",
    "            Z = self.block1(Z)\n",
    "        Z = self.block2(Z)\n",
    "        Z = self.out(Z)\n",
    "        return Z\n",
    "    \n",
    "#   再構築ロス : 隠れ層の重みなどの補助出力を行う\n",
    "#   再構築ロスの計算を含むカスタムモデル\n",
    "class ReconstructingRegressor(keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\") for _ in range(5)]\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "        \n",
    "    def build(self, batch_input_shape):\n",
    "        n_inputs = batch_input_shape[-1]\n",
    "        self.reconstruct = keras.layers.Dense(n_inputs)\n",
    "        super().build(batch_input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        reconstruction = self.reconstruct(Z)\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))   #   再構築データと入力との平均二乗誤差\n",
    "        self.add_loss(0.05 * recon_loss)    #   損失リストに含める\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=0.5>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#   自動微分\n",
    "def f(w1, w2):\n",
    "    return 3*w1**2 + 2*w1*w2\n",
    "\n",
    "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "    \n",
    "gradients = tape.gradient(z, [w1, w2])  #   w1, w2で偏微分した結果をtf.Tensor型の1*2行列を出力\n",
    "#   tape.gradient(z, [w1, w2])  #   error! tapeはgradient()が呼ばれた直後に自動で消去される\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:  #   tapeは永続化させられる\n",
    "    z = f(w1, w2)\n",
    "dz_dw1 = tape.gradient(z, w1)\n",
    "dz_dw2 = tape.gradient(z, w2)\n",
    "del tape\n",
    "\n",
    "c1, c2 = tf.constant(5.), tf.constant(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(c1, c2)\n",
    "gradients = tape.gradient(z, [c1, c2])  #   変数以外のとき勾配を計算するとNoneが出力される\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(c1)  #   任意のテンソルを監視するよう指示すると、変数であるかのようにふるまわせることができる\n",
    "    tape.watch(c2)\n",
    "    z = f(c1, c2)\n",
    "\n",
    "gradients = tape.gradient(z, [c1, c2])  #   [tensor 36., tensor 10.]を出力\n",
    "\n",
    "def g(w1, w2):\n",
    "    return 3*w1**2 + tf.stop_gradient(2*w1*w2)  #   stop_gradient()により定数としてふるまうので、w1で偏微分すると6w1となるイメージ\n",
    "with tf.GradientTape() as tape:\n",
    "    z = g(w1, w2)\n",
    "gradients = tape.gradient(z, [w1, w2])  #   [tensor 30., tensor None]\n",
    "\n",
    "x = tf.Variable(100.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_softplus(x)\n",
    "tape.gradient(z, [x])    #   [NaN] 自動微分では計算不可な場合がある。\n",
    "\n",
    "#   上の問題の対策として、関数をデコレートして通常の出力に加えて導関数を返すように編集できる\n",
    "@tf.custom_gradient\n",
    "def my_better_softplus(z):\n",
    "    exp = tf.exp(z)\n",
    "    def my_softplus_gradients(grad):\n",
    "        return grad / (1 + 1 / exp)\n",
    "    return tf.math.log(exp + 1), my_softplus_gradients\n",
    "\n",
    "x = tf.Variable(0.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_better_softplus(x)\n",
    "tape.gradient(z, [x])    #   0.5となり計算ができるようになる\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   カスタム訓練ループ : fit()を使わず実装する戦略\n",
    "l2_reg = keras.regularizers.l2(0.05)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\", kernel_regularizer=l2_reg),\n",
    "    keras.layers.Dense(1, kernel_regularizer=l2_reg)\n",
    "])\n",
    "\n",
    "def random_batch(X, y, batch_size=32):\n",
    "    idx = np.random.randint(len(X), size=batch_size)\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "def print_status_bar(iteration, total, loss, metrics=None):\n",
    "    metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result())\n",
    "                          for m in [loss] + (metrics or [])])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{}/{} - \".format(iteration, total) + metrics, end=end)\n",
    "\n",
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size    #   整数除算\n",
    "optimizer = keras.optimizers.Nadam(lr=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "mean_loss = keras.metrics.Mean()\n",
    "metrics = [keras.metrics.MeanAbsoluteError()]\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):    #   epochループ\n",
    "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
    "    for step in range(1, n_steps + 1):  #   epoch内のバッチループ\n",
    "        X_batch, y_batch = random_batch(X_train_scaled, y_train)    #   バッチを抽出\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch, training=True)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses) #   正則化ロス\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))    #   tapeによって計算した勾配をoptimizerに適用\n",
    "        for variable in model.variables:    #   重みに制約を加える\n",
    "            if variable.constraint is not None:\n",
    "                variable.assign(variable.constraint(variable))\n",
    "        mean_loss(loss) #   平均損失の更新\n",
    "        for metric in metrics:  #   指標の更新\n",
    "            metric(y_batch, y_pred)\n",
    "        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\n",
    "    print_status_bar(len(y_train), len(y_train), mean_loss, metrics)\n",
    "    for metric in [mean_loss] + metrics:    #   リセットして次のepochへ\n",
    "        metric.reset_states()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.def_function.Function at 0x27f8a78e0e0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#   tensorflow関数とグラフ\n",
    "#   tensorflow関数は計算グラフを効率よく実行するのでpython関数より高速。複雑な計算には有効。\n",
    "def cube(x):\n",
    "    return x**3\n",
    "tf_cube = tf.function(cube) #   python関数をtensorflow関数に変換\n",
    "#   カスタムコンポーネントをkerasで使用したときは自動的にtensorflow関数に変換されるので、tf.functionを使う必要がない\n",
    "#   カスタムコンポーネントを作るときにdynamic=Trueを指定するか、compile()時にrun_eagerly=Trueを指定することでこの変換を防げる\n",
    "\n",
    "#   tensorflow関数の注意\n",
    "#   外部ライブラリはグラフの一部にはならないので、np.sum()よりもtf.reduce_sum()、sorted()よりtf.sort()を使うべき\n",
    "#   tensorflowはテンソルかデータセットを処理するfor文しか捕捉しないので、for i in range()よりfor i in tf.range()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a6fdf0cebff75ebc8562d1fd6dfb543976b20263b8527b00aa8ceb00671f916a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
